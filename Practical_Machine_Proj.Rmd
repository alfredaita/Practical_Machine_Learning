---
title: "Practical Machine Learning"
author: "Alfred Aita"
date: "November 14, 2015"
output: 
   html_document:
     keep_md: TRUE
---
# Predicting Excersise Quality  

## Mission and Motivation

 The main focus of this project is to build a Machine learning model which will predict the quality of the weight lifting exercise being performed by 6 participants,wearing digital sensors designed to measure their activity (accelerometers).The exercise performance is qualified as: (A,B,C,D,E).
 
 Our data sources for training and testing are Downloaded  at:
 
 https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv  
 
 https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv

Thanks to groupware for sharing the data and supporting documentation 

http://groupware.les.inf.puc-rio.br/har

# Dats Process and first steps

>- After downloading the two data sets  are loaded and a first exploratory review is performed.
>- Our examination begins and we note their are 159 potential predictors and one outcome: 'classe'.
>- We note some variables contain many NAs and not needed. We eliminate those
>- Additional data process steps are performed and we are able to reduce  to 53 predictors.
>- We further examine our data with some exploritory Data analysis.
>- Using the caret package, we  split the training set into two sets training and testing.
>- We now build the model for classification using random forest with cross-validation.
>- The model is trained and summary information and a confusion table are examined. 
>- Predictions are made using the model on the out Of Sample test set (OOS)
>- OOS Accuracy and estimated error rates calculated.
>- The  model is then applied to our downloaded test data for answering questions.
>- Final thoughts on the project
>- Predictions are written to a text file for the write up.




```{r,cache=TRUE}
Rawtraining <-read.csv("pml-training.csv", na.strings = c("NA", ""))
Rawtesting<-read.csv("pml-testing.csv",na.strings = c("NA","")) 
dim(Rawtraining);dim(Rawtesting)
```

we can see that there are probably too many variables,next we will eliminate the Na for both sets by 
column and again examine our dimension. The goal is to cut down on variables but not observations.  

```{r} 
# ID the column with too many NAs
NAs<- apply(Rawtraining,2,function(x){sum(is.na(x))})
# new data set
Rawtraining <- Rawtraining[,which(NAs==0)] # purge them

NAs<- apply(Rawtesting,2,function(x){sum(is.na(x))})
Rawtesting <- Rawtesting[,which(NAs==0)] 
dim(Rawtraining);dim(Rawtesting)
```
Lets take a look at what else we can eliminate. We only need to examine the first few. 

```{r,cache=F}
str(list(head(Rawtraining,4)),list.len = 8)
```


By examining the above we can see an additional 6 more variables can be omitted. They probably would add
no predictive advantages.

```{r,cache=F} 

throwOut<-grep("timestamp|X|user_name|new_window", names(Rawtraining))
TRaining<-Rawtraining[,-throwOut]; TEsting<- Rawtesting[,-throwOut]
dim(TRaining); dim(TEsting)  
```

Now split the training data into testing and training note: **testing** is all small case

```{r}
library(caret)
set.seed(123) # so we can reproduce 
inTrain<- createDataPartition(y=TRaining$classe,p= 0.7,list = F)
training <- TRaining[inTrain,]; testing <- TRaining[-inTrain,]
dim(training);dim(testing)
```

there are several variables which are totals of the others lets plot them for some insight

```{r}
Accel<- grep("total",names(training))
  #AccelFrame<- training[,Accel]
  #transparentTheme(trans = .9)
featurePlot(x=training[,Accel],
            y=training$classe,  
            plot = "density",
            scales = list(x=list(relation = "free"),
                          y= list(relation = "free")),
                          adjust = 1.5,pch ="|", layout =c(2,2),
                          auto.key =list(columns = 3))

```
 
# Machine learning algorythm

Now build a classification model, random forest with cross validation. The model take quite sometime to run so we will cache it.


```{r,cache = TRUE}
#A train control function is created to set up cross validation
Fitcontrol<- trainControl(method = "cv",number = 3)

# training the model
Rf.model<- train(training$classe ~., method= "rf",trControl = Fitcontrol,data = training)
summary(Rf.model)
```


## Results from the trained model  

Now look at models stats and a confusion matrix 


```{r}
Rf.model; Rf.model$finalModel$confusion 
```

Our test data has been processed so lets predict. Recall this set is named **TEsting** 
 Applying our machine learning model to get predictions.
```{r}
  prediction <- predict(Rf.model,newdata = TEsting)

prediction;  print('prediction summary'); summary(prediction)
```


 We will now examine some charts of our model. Lets look at accuracy compared to the number of predictors (log)

Our plot confirms that the the best model is the second indicated from above with 27 variables.

```{r,cache= F}
plot(Rf.model, log = "y")
```

# Out of sample error rate 

Now we will use test set partitioned from our training set.(I.E testing)

```{r}
predicted <- predict(Rf.model,newdata= testing)
length(predicted)
testSetAccuracy <- sum(predicted ==testing$classe )/length(predicted)
round(testSetAccuracy,6)
OOSerrorRate <-  (1-testSetAccuracy)
OOSerrorRate 
paste0("The out of sample error rate estimate is",round((OOSerrorRate *100),2),"%")
```

## Final thought
 More times then not our out of sample error would be higher, this model however, did slightly better.
 Although by an insignificant amount.
 Why? The luck of the seed perhaps.
 
## Capture file for write up

```{r}
write_PML = function(x){
       n =length(x)  
       for(i in 1:n){
         filename = paste0("problem_id_", i, ".txt")
         write.table(x[i], file = filename, quote = FALSE, row.names = FALSE, 
            col.names = FALSE)}
}
write_PML(prediction)
```
